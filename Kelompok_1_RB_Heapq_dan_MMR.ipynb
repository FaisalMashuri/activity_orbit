{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kelompok 1 RB_Heapq dan MMR.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaisalMashuri/activity_orbit/blob/main/Kelompok_1_RB_Heapq_dan_MMR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohOG_ahTnm4c"
      },
      "source": [
        "### Heapq dengan menggunakan bahasa Indonesia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5YBuDN5KN5l",
        "outputId": "ff011041-e89c-4a0a-adf9-26ba68028fce"
      },
      "source": [
        "!pip install sastrawi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▋                              | 10 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 30 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 40 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 51 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 61 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 71 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 81 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 102 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 112 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 122 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 133 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 143 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 153 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 163 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 174 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 184 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 194 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 204 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 209 kB 7.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: sastrawi\n",
            "Successfully installed sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiLhAJY7nis4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eli9g8y2NUrB",
        "outputId": "0c8ccc29-9618-4206-ebe0-443dc15b1d29"
      },
      "source": [
        "!pip install newspaper3k"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 20.1 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 71 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 81 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 102 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 122 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 133 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 143 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 153 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 163 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 174 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 194 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 204 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 211 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting jieba3k>=0.35.1\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 16.7 MB/s \n",
            "\u001b[?25hCollecting feedparser>=5.2.1\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "Collecting tldextract>=2.0.1\n",
            "  Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.3.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13552 sha256=5d83b758199de20a8cc65de5288f10e96e4850ab8a867aaa8f2f980bb238e213\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3356 sha256=e9d3dcdc51673f7e52ce1e103fb9bbdb9934027bfce27ce2ce180532edd1e1ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398405 sha256=86a18fb4ba31db3ee0af0c228f23e73a30e81b4ffb9728c0749a2681c2ce1dee\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=0e7e4b849432e15059d1f0264a48bf796392a6946e6be3ba3c3c30988055d92c\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: sgmllib3k, requests-file, tldextract, tinysegmenter, jieba3k, feedparser, feedfinder2, cssselect, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.8 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dXYFhvIKrRp"
      },
      "source": [
        "import Sastrawi\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from string import punctuation\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb8mA4E2QIF_"
      },
      "source": [
        "pip install spacy>2.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G2p7WqKMl2k",
        "outputId": "11140173-1647-4c36-c120-be9f281c9187"
      },
      "source": [
        "stop_factory = StopWordRemoverFactory()\n",
        "more_stopword = ['dengan', 'ia','bahwa','oleh’']\n",
        "data = stop_factory.get_stop_words()+more_stopword\n",
        "stopword = stop_factory.create_stop_word_remover()\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'dia', 'dua', 'ia', 'seperti', 'jika', 'jika', 'sehingga', 'kembali', 'dan', 'tidak', 'ini', 'karena', 'kepada', 'oleh', 'saat', 'harus', 'sementara', 'setelah', 'belum', 'kami', 'sekitar', 'bagi', 'serta', 'di', 'dari', 'telah', 'sebagai', 'masih', 'hal', 'ketika', 'adalah', 'itu', 'dalam', 'bisa', 'bahwa', 'atau', 'hanya', 'kita', 'dengan', 'akan', 'juga', 'ada', 'mereka', 'sudah', 'saya', 'terhadap', 'secara', 'agar', 'lain', 'anda', 'begitu', 'mengapa', 'kenapa', 'yaitu', 'yakni', 'daripada', 'itulah', 'lagi', 'maka', 'tentang', 'demi', 'dimana', 'kemana', 'pula', 'sambil', 'sebelum', 'sesudah', 'supaya', 'guna', 'kah', 'pun', 'sampai', 'sedangkan', 'selagi', 'sementara', 'tetapi', 'apakah', 'kecuali', 'sebab', 'selain', 'seolah', 'seraya', 'seterusnya', 'tanpa', 'agak', 'boleh', 'dapat', 'dsb', 'dst', 'dll', 'dahulu', 'dulunya', 'anu', 'demikian', 'tapi', 'ingin', 'juga', 'nggak', 'mari', 'nanti', 'melainkan', 'oh', 'ok', 'seharusnya', 'sebetulnya', 'setiap', 'setidaknya', 'sesuatu', 'pasti', 'saja', 'toh', 'ya', 'walau', 'tolong', 'tentu', 'amat', 'apalagi', 'bagaimanapun', 'dengan', 'ia', 'bahwa', 'oleh’']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBB1UxO_N89Z",
        "outputId": "aceea4e2-5fa1-4ea9-c99f-d3d06829104f"
      },
      "source": [
        " import nltk\n",
        " nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yiDRMvssNVZ",
        "outputId": "4fb940c7-1568-40d2-8155-7aeb5ca891ce"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "print(nlp) # Object"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<spacy.lang.en.English object at 0x7f8cee62e3d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CvYk8PHMtWw"
      },
      "source": [
        "text = \"\"\" Hingga sekarang, pandemi sudah melanda selama kurang lebih 13 bulan. Aku yakin banyak dari kita yang merasa suntuk, cape, malas, dan lain-lain. Kita ga bisa terus-terusan begini kan? Nantinya, yang akan menjadi dampak adalah mental kita. Oleh karena itu kita harus bisa menjaga kesehatan mental di masa pandemi ini.\n",
        "Seperti yang aku katakan di awal bahwa orang yang mengalami masalah kesehatan mental terus meningkat setiap harinya di masa pandemi ini. Bagaimana tidak? Jumlah psikolog Indonesia jauh lebih sedikit dibanding jumlah orang yang menderita penyakit mental di Indonesia.\n",
        "Namun, aku hadir membawa solusi. Solusinya ada dari diri kita sendiri. Yaps! Diri kitalah yang harus menjaga kesehatan mental kita sendiri. Caranya gimana? Baca sampai habis yaa! Without mental health there can be no true physical health\n",
        "- Dr. Brock Chisholm\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2vRODjLQi6X",
        "outputId": "9dc0d1db-a4d8-4cc5-dd90-7d0cb8b9d5a7"
      },
      "source": [
        "doc = nlp(text)\n",
        "doc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " Hingga sekarang, pandemi sudah melanda selama kurang lebih 13 bulan. Aku yakin banyak dari kita yang merasa suntuk, cape, malas, dan lain-lain. Kita ga bisa terus-terusan begini kan? Nantinya, yang akan menjadi dampak adalah mental kita. Oleh karena itu kita harus bisa menjaga kesehatan mental di masa pandemi ini.\n",
              "Seperti yang aku katakan di awal bahwa orang yang mengalami masalah kesehatan mental terus meningkat setiap harinya di masa pandemi ini. Bagaimana tidak? Jumlah psikolog Indonesia jauh lebih sedikit dibanding jumlah orang yang menderita penyakit mental di Indonesia.\n",
              "Namun, aku hadir membawa solusi. Solusinya ada dari diri kita sendiri. Yaps! Diri kitalah yang harus menjaga kesehatan mental kita sendiri. Caranya gimana? Baca sampai habis yaa! Without mental health there can be no true physical health\n",
              "- Dr. Brock Chisholm"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKnd1ixMP248",
        "outputId": "919e11bb-044d-418b-ca41-38fe0f73e96b"
      },
      "source": [
        "nlp = spacy.blank('id')\n",
        "print(nlp) # Object"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<spacy.lang.id.Indonesian object at 0x7f8ced01fb10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3ePrQBPNot6",
        "outputId": "947338b7-7475-4d5c-e706-2fc61ef589f3"
      },
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', 'Hingga', 'sekarang', ',', 'pandemi', 'sudah', 'melanda', 'selama', 'kurang', 'lebih', '13', 'bulan', '.', 'Aku', 'yakin', 'banyak', 'dari', 'kita', 'yang', 'merasa', 'suntuk', ',', 'cape', ',', 'malas', ',', 'dan', 'lain', '-', 'lain', '.', 'Kita', 'ga', 'bisa', 'terus', '-', 'terusan', 'begini', 'kan', '?', 'Nantinya', ',', 'yang', 'akan', 'menjadi', 'dampak', 'adalah', 'mental', 'kita', '.', 'Oleh', 'karena', 'itu', 'kita', 'harus', 'bisa', 'menjaga', 'kesehatan', 'mental', 'di', 'masa', 'pandemi', 'ini', '.', '\\n', 'Seperti', 'yang', 'aku', 'katakan', 'di', 'awal', 'bahwa', 'orang', 'yang', 'mengalami', 'masalah', 'kesehatan', 'mental', 'terus', 'meningkat', 'setiap', 'harinya', 'di', 'masa', 'pandemi', 'ini', '.', 'Bagaimana', 'tidak', '?', 'Jumlah', 'psikolog', 'Indonesia', 'jauh', 'lebih', 'sedikit', 'dibanding', 'jumlah', 'orang', 'yang', 'menderita', 'penyakit', 'mental', 'di', 'Indonesia', '.', '\\n', 'Namun', ',', 'aku', 'hadir', 'membawa', 'solusi', '.', 'Solusinya', 'ada', 'dari', 'diri', 'kita', 'sendiri', '.', 'Yaps', '!', 'Diri', 'kitalah', 'yang', 'harus', 'menjaga', 'kesehatan', 'mental', 'kita', 'sendiri', '.', 'Caranya', 'gimana', '?', 'Baca', 'sampai', 'habis', 'yaa', '!', 'Without', 'mental', 'health', 'there', 'can', 'be', 'no', 'true', 'physical', 'health', '\\n', '-', 'Dr.', 'Brock', 'Chisholm', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "C3hJlo6CNrIg",
        "outputId": "dbe2b7a7-f84e-4b84-e395-fd0edc955303"
      },
      "source": [
        "punctuation = punctuation + '\\n'\n",
        "punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLBxIsl0Q47s",
        "outputId": "44fd572e-0294-400e-91ac-3187fd1adcb3"
      },
      "source": [
        "# Membuat dictionary bag of word\n",
        "word_frequencies = {}\n",
        "\n",
        "# Mengisi word_frequencies tanpa stopword dan karakter khusus\n",
        "for word in doc:\n",
        "    if word.text.lower() not in data:\n",
        "        if word.text.lower() not in punctuation:\n",
        "            if word.text not in word_frequencies.keys():\n",
        "                word_frequencies[word.text] = 1\n",
        "            else:\n",
        "                word_frequencies[word.text] += 1\n",
        "                \n",
        "print(word_frequencies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 1, 'Hingga': 1, 'sekarang': 1, 'pandemi': 3, 'melanda': 1, 'selama': 1, 'kurang': 1, 'lebih': 2, '13': 1, 'bulan': 1, 'Aku': 1, 'yakin': 1, 'banyak': 1, 'merasa': 1, 'suntuk': 1, 'cape': 1, 'malas': 1, 'ga': 1, 'terus': 2, 'terusan': 1, 'begini': 1, 'kan': 1, 'Nantinya': 1, 'menjadi': 1, 'dampak': 1, 'mental': 6, 'menjaga': 2, 'kesehatan': 3, 'masa': 2, 'aku': 2, 'katakan': 1, 'awal': 1, 'orang': 2, 'mengalami': 1, 'masalah': 1, 'meningkat': 1, 'harinya': 1, 'Bagaimana': 1, 'Jumlah': 1, 'psikolog': 1, 'Indonesia': 2, 'jauh': 1, 'sedikit': 1, 'dibanding': 1, 'jumlah': 1, 'menderita': 1, 'penyakit': 1, 'hadir': 1, 'membawa': 1, 'solusi': 1, 'Solusinya': 1, 'diri': 1, 'sendiri': 2, 'Yaps': 1, 'Diri': 1, 'kitalah': 1, 'Caranya': 1, 'gimana': 1, 'Baca': 1, 'habis': 1, 'yaa': 1, 'Without': 1, 'health': 2, 'there': 1, 'can': 1, 'be': 1, 'no': 1, 'true': 1, 'physical': 1, 'Dr.': 1, 'Brock': 1, 'Chisholm': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y78282CzRG-S",
        "outputId": "91a85113-9262-4b6d-95f6-cfb8569f323d"
      },
      "source": [
        "max_frequency = max(word_frequencies.values())\n",
        "max_frequency"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_7YP9J8RLe0",
        "outputId": "2780ccee-8631-4ebd-cb28-a9e02f96cdfa"
      },
      "source": [
        "for word in word_frequencies.keys():\n",
        "    word_frequencies[word] = word_frequencies[word]/max_frequency\n",
        "\n",
        "print(word_frequencies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0.16666666666666666, 'Hingga': 0.16666666666666666, 'sekarang': 0.16666666666666666, 'pandemi': 0.5, 'melanda': 0.16666666666666666, 'selama': 0.16666666666666666, 'kurang': 0.16666666666666666, 'lebih': 0.3333333333333333, '13': 0.16666666666666666, 'bulan': 0.16666666666666666, 'Aku': 0.16666666666666666, 'yakin': 0.16666666666666666, 'banyak': 0.16666666666666666, 'merasa': 0.16666666666666666, 'suntuk': 0.16666666666666666, 'cape': 0.16666666666666666, 'malas': 0.16666666666666666, 'ga': 0.16666666666666666, 'terus': 0.3333333333333333, 'terusan': 0.16666666666666666, 'begini': 0.16666666666666666, 'kan': 0.16666666666666666, 'Nantinya': 0.16666666666666666, 'menjadi': 0.16666666666666666, 'dampak': 0.16666666666666666, 'mental': 1.0, 'menjaga': 0.3333333333333333, 'kesehatan': 0.5, 'masa': 0.3333333333333333, 'aku': 0.3333333333333333, 'katakan': 0.16666666666666666, 'awal': 0.16666666666666666, 'orang': 0.3333333333333333, 'mengalami': 0.16666666666666666, 'masalah': 0.16666666666666666, 'meningkat': 0.16666666666666666, 'harinya': 0.16666666666666666, 'Bagaimana': 0.16666666666666666, 'Jumlah': 0.16666666666666666, 'psikolog': 0.16666666666666666, 'Indonesia': 0.3333333333333333, 'jauh': 0.16666666666666666, 'sedikit': 0.16666666666666666, 'dibanding': 0.16666666666666666, 'jumlah': 0.16666666666666666, 'menderita': 0.16666666666666666, 'penyakit': 0.16666666666666666, 'hadir': 0.16666666666666666, 'membawa': 0.16666666666666666, 'solusi': 0.16666666666666666, 'Solusinya': 0.16666666666666666, 'diri': 0.16666666666666666, 'sendiri': 0.3333333333333333, 'Yaps': 0.16666666666666666, 'Diri': 0.16666666666666666, 'kitalah': 0.16666666666666666, 'Caranya': 0.16666666666666666, 'gimana': 0.16666666666666666, 'Baca': 0.16666666666666666, 'habis': 0.16666666666666666, 'yaa': 0.16666666666666666, 'Without': 0.16666666666666666, 'health': 0.3333333333333333, 'there': 0.16666666666666666, 'can': 0.16666666666666666, 'be': 0.16666666666666666, 'no': 0.16666666666666666, 'true': 0.16666666666666666, 'physical': 0.16666666666666666, 'Dr.': 0.16666666666666666, 'Brock': 0.16666666666666666, 'Chisholm': 0.16666666666666666}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nR1pm8fSVJ4u",
        "outputId": "aa4e7644-f2b5-4e3c-97c1-ce0085bf4912"
      },
      "source": [
        "word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Chisholm'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H72ayeq0ROEi",
        "outputId": "e35bb3b0-d5c3-4a37-916e-a0b30cb463e9"
      },
      "source": [
        "sentence_tokens = [sent for sent in text.split('.')]\n",
        "print(sentence_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' Hingga sekarang, pandemi sudah melanda selama kurang lebih 13 bulan', ' Aku yakin banyak dari kita yang merasa suntuk, cape, malas, dan lain-lain', ' Kita ga bisa terus-terusan begini kan? Nantinya, yang akan menjadi dampak adalah mental kita', ' Oleh karena itu kita harus bisa menjaga kesehatan mental di masa pandemi ini', '\\nSeperti yang aku katakan di awal bahwa orang yang mengalami masalah kesehatan mental terus meningkat setiap harinya di masa pandemi ini', ' Bagaimana tidak? Jumlah psikolog Indonesia jauh lebih sedikit dibanding jumlah orang yang menderita penyakit mental di Indonesia', '\\nNamun, aku hadir membawa solusi', ' Solusinya ada dari diri kita sendiri', ' Yaps! Diri kitalah yang harus menjaga kesehatan mental kita sendiri', ' Caranya gimana? Baca sampai habis yaa! Without mental health there can be no true physical health\\n- Dr', ' Brock Chisholm\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k8uHgn8UK93",
        "outputId": "142b21f2-6014-447e-8858-e1fa1b073cd3"
      },
      "source": [
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "    for word in sent:\n",
        "        if word.lower() in word_frequencies.keys():\n",
        "            if sent not in sentence_scores.keys():\n",
        "                sentence_scores[sent] = word_frequencies[word.lower()]\n",
        "            else:\n",
        "                sentence_scores[sent] += word_frequencies[word.lower()]\n",
        "                \n",
        "sentence_scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\nNamun, aku hadir membawa solusi': 0.6666666666666666,\n",
              " '\\nSeperti yang aku katakan di awal bahwa orang yang mengalami masalah kesehatan mental terus meningkat setiap harinya di masa pandemi ini': 3.333333333333332,\n",
              " ' Aku yakin banyak dari kita yang merasa suntuk, cape, malas, dan lain-lain': 2.0,\n",
              " ' Bagaimana tidak? Jumlah psikolog Indonesia jauh lebih sedikit dibanding jumlah orang yang menderita penyakit mental di Indonesia': 2.8333333333333326,\n",
              " ' Brock Chisholm\\n': 0.3333333333333333,\n",
              " ' Caranya gimana? Baca sampai habis yaa! Without mental health there can be no true physical health\\n- Dr': 2.8333333333333326,\n",
              " ' Hingga sekarang, pandemi sudah melanda selama kurang lebih 13 bulan': 1.6666666666666667,\n",
              " ' Kita ga bisa terus-terusan begini kan? Nantinya, yang akan menjadi dampak adalah mental kita': 2.333333333333333,\n",
              " ' Oleh karena itu kita harus bisa menjaga kesehatan mental di masa pandemi ini': 2.1666666666666665,\n",
              " ' Solusinya ada dari diri kita sendiri': 0.9999999999999999,\n",
              " ' Yaps! Diri kitalah yang harus menjaga kesehatan mental kita sendiri': 1.6666666666666667}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1xahjOcRQRQ"
      },
      "source": [
        "from heapq import nlargest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfW3WG0dU0U2",
        "outputId": "c3f4a124-ed34-44ee-9e9c-6b149e85dcb1"
      },
      "source": [
        "summarization_percentage = 0.3\n",
        "select_length = int(len(sentence_tokens) * summarization_percentage)\n",
        "select_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va8WFsmLVng_"
      },
      "source": [
        "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW1Oyk3_VrTa",
        "outputId": "91e13233-dbfe-44c5-c151-b140de424592"
      },
      "source": [
        "summary\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nSeperti yang aku katakan di awal bahwa orang yang mengalami masalah kesehatan mental terus meningkat setiap harinya di masa pandemi ini',\n",
              " ' Bagaimana tidak? Jumlah psikolog Indonesia jauh lebih sedikit dibanding jumlah orang yang menderita penyakit mental di Indonesia',\n",
              " ' Caranya gimana? Baca sampai habis yaa! Without mental health there can be no true physical health\\n- Dr']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZzx2JKFVtAl"
      },
      "source": [
        "final_summary = [word for word in summary]\n",
        "summary = ' '.join(final_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "TSUe9A5_Vupr",
        "outputId": "26b1df64-ff70-4114-d7e4-c4f4545c254e"
      },
      "source": [
        "summary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nSeperti yang aku katakan di awal bahwa orang yang mengalami masalah kesehatan mental terus meningkat setiap harinya di masa pandemi ini  Bagaimana tidak? Jumlah psikolog Indonesia jauh lebih sedikit dibanding jumlah orang yang menderita penyakit mental di Indonesia  Caranya gimana? Baca sampai habis yaa! Without mental health there can be no true physical health\\n- Dr'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPeGC1O6VyQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3faa55c-3f53-418b-e1a0-5969c66ced1f"
      },
      "source": [
        "print(f\"Jumlah kata sebelum dirangkum: {len(text)}\")\n",
        "print(f\"Jumlah kata setelah dirangkum: {len(summary)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah kata sebelum dirangkum: 843\n",
            "Jumlah kata setelah dirangkum: 370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laeFJ761V0H1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn4NbItboHzD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMpZ-bBdn-kj"
      },
      "source": [
        "### MMR dengan bahasa Inggris"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jc9x511oDHY"
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "import spacy\n",
        "# from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "\n",
        "import re   # reguler ekspresi\n",
        "# import sys  # untuk argv (tapi untuk Linux dan lokal)\n",
        "import requests # untuk reques data dari repo github misalnya\n",
        "from sklearn.feature_extraction.text import CountVectorizer # Vector \n",
        "from sklearn.metrics.pairwise import cosine_similarity # cosine similarity\n",
        "\n",
        "import operator # operator artimatika dll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr8ChXSpoJZ7",
        "outputId": "373ccaf1-4563-483a-ec41-6833312aae37"
      },
      "source": [
        "stopwords = list(STOP_WORDS)\n",
        "print(stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'ve\", 'not', 'latter', 'an', '‘d', 'due', 'everything', 'some', 'noone', 'fifty', 'seem', 'somehow', 'to', 'show', 'part', 'beside', \"'d\", 'whither', 'anyhow', 'up', 'amount', 'another', 'except', 'herein', 'any', 'ca', 'doing', 'anyway', 'when', 'herself', 'third', 'perhaps', 'every', 'has', 'least', 'many', 'i', 'top', 'who', 'upon', 'make', 'out', 'those', 'although', 'others', 'elsewhere', 'beforehand', 'at', 'hence', 'what', 'whole', 'sixty', 'afterwards', 'beyond', 'your', 'along', 'rather', 'eight', 'via', 'whom', 'towards', 'me', 'hundred', 'over', \"'s\", 'did', 'been', '‘m', 'with', 'is', 'please', 'they', 'all', 'about', 'last', 'hereafter', 'two', 'various', 'yours', 'are', 'below', 'yourselves', 'before', 'eleven', 'wherein', 'themselves', 'together', 'bottom', 'unless', 'mostly', 'therein', 'seeming', 'serious', 'whereupon', 're', 'more', 'empty', 'hers', \"n't\", 'seems', 'next', 'front', 'thus', 'one', 'a', 'these', 'done', 'twelve', 'whatever', 'nothing', 'name', '‘ll', 'back', 'nowhere', 'ten', 'our', '’re', 'namely', '’d', \"'ll\", 'whereas', 'everywhere', 'into', 'alone', 'until', 'further', 'should', 'can', 'whoever', 'three', 'whether', 'must', 'side', '’ll', 'down', 'cannot', 'became', 'none', 'put', 'never', 'besides', 'whereby', 'anything', 'else', 'most', 'same', 'where', 'much', 'becoming', 'nine', 'sometime', 'n‘t', 'while', 'be', 'because', 'as', 'often', 'other', 'thru', 'made', 'himself', 'meanwhile', 'seemed', 'than', 'then', 'onto', 'it', 'would', 'n’t', 'her', 'whereafter', 'their', 'was', 'take', 'already', 'this', '‘s', 'also', 'moreover', '’m', 'yet', '’ve', 'us', 'ours', '’s', 'do', 'if', 'now', 'whence', 'sometimes', 'amongst', 'thereby', 'nobody', 'nevertheless', 'once', 'thereupon', 'that', 'its', 'still', 'between', 'always', 'am', 'even', 'throughout', 'could', 'or', 'does', 'toward', 'well', 'no', 'again', 'his', 'six', 'the', 'them', 'you', '‘ve', 'how', 'very', 'several', 'my', 'twenty', 'few', 'across', 'ourselves', 'myself', 'less', 'quite', 'becomes', 'might', 'both', 'itself', 'call', 'first', 'per', 'almost', 'go', 'such', 'just', 'get', 'will', 'so', 'have', 'move', 'on', 'among', 'something', 'of', '‘re', 'own', 'but', 'yourself', 'either', 'had', 'there', 'hereby', 'anyone', 'wherever', 'enough', 'each', 'five', 'used', 'using', 'from', 'really', 'whenever', 'for', \"'re\", 'him', 'regarding', 'neither', 'nor', 'around', 'by', 'under', 'mine', 'indeed', 'everyone', 'were', 'former', 'she', \"'m\", 'whose', 'which', 'through', 'keep', 'see', 'latterly', 'here', 'four', 'though', 'within', 'he', 'give', 'too', 'thereafter', 'forty', 'only', 'why', 'after', 'full', 'formerly', 'anywhere', 'fifteen', 'therefore', 'behind', 'ever', 'hereupon', 'without', 'may', 'otherwise', 'become', 'say', 'since', 'someone', 'thence', 'above', 'being', 'and', 'during', 'in', 'against', 'off', 'we', 'however', 'somewhere']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pRcdbsfoL5_",
        "outputId": "546ee8bd-6ec9-4b30-d22a-2b872a97747e"
      },
      "source": [
        "# Mengambil komponen - komponen pada Spacy seperti:\n",
        "# tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "print(nlp) # Object"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<spacy.lang.en.English object at 0x7f8ced19e950>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg5DD8ZWoOXl",
        "outputId": "bcfb58ae-6365-4a43-9a09-5552316e76ff"
      },
      "source": [
        "!pip install newspaper3k\n",
        "from newspaper import Article\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.7/dist-packages (0.2.8)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (1.1.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.1.2)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (6.0.8)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2021.5.30)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.3.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh1Kg5mcoSu1",
        "outputId": "acbca7c5-a1cb-4f6d-959f-5d7bd9d67415"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE37KY9ioUzv"
      },
      "source": [
        "article1 = Article('https://towardsdatascience.com/nlp-for-beginners-cleaning-preprocessing-text-data-ae8e306bef0f')\n",
        "article1.download()\n",
        "article1.parse()\n",
        "article1.nlp()\n",
        "# print(type(article1.text))\n",
        "texts = article1.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ntr5yrXoXgV",
        "outputId": "e8d72825-d38e-4313-922c-fa7956cc187b"
      },
      "source": [
        "doc = nlp(texts)\n",
        "print(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP is short for Natural Language Processing. As you probably know, computers are not as great at understanding words as they are numbers. This is all changing though as advances in NLP are happening everyday. The fact that devices like Apple’s Siri and Amazon’s Alexa can (usually) comprehend when we ask the weather, for directions, or to play a certain genre of music are all examples of NLP. The spam filter in your email and the spellcheck you’ve used since you learned to type in elementary school are some other basic examples of when your computer is understanding language.\n",
            "\n",
            "As a data scientist, we may use NLP for sentiment analysis (classifying words to have positive or negative connotation) or to make predictions in classification models, among other things. Typically, whether we’re given the data or have to scrape it, the text will be in its natural human format of sentences, paragraphs, tweets, etc. From there, before we can dig into analyzing, we will have to do some cleaning to break the text down into a format the computer can easily understand.\n",
            "\n",
            "For this example, we’re examining a dataset of Amazon products/reviews which can be found and downloaded for free on data.world. I’ll be using Python in Jupyter notebook.\n",
            "\n",
            "Here are the imports used:\n",
            "\n",
            "(You may need to run nltk.download() in a cell if you’ve never previously used it.)\n",
            "\n",
            "Read in csv file, create DataFrame & check shape. We are starting out with 10,000 rows and 17 columns. Each row is a different product on Amazon.\n",
            "\n",
            "I conducted some basic data cleaning that I won’t go into detail about now, but you can read my post about EDA here if you want some tips.\n",
            "\n",
            "In order to make the dataset more manageable for this example, I first dropped columns with too many nulls and then dropped any remaining rows with null values. I changed the number_of_reviews column type from object to integer and then created a new DataFrame using only the rows with no more than 1 review. My new shape is 3,705 rows and 10 columns and I renamed it reviews_df .\n",
            "\n",
            "NOTE: If we were actually going to use this dataset for analysis or modeling or anything besides a text preprocessing demo, I would not recommend eliminating such a large percent of the rows.\n",
            "\n",
            "The following workflow is what I was taught to use and like using, but the steps are just general suggestions to get you started. Usually I have to modify and/or expand depending on the text format.\n",
            "\n",
            "Remove HTML Tokenization + Remove punctuation Remove stop words Lemmatization or Stemming\n",
            "\n",
            "While cleaning this data I ran into a problem I had not encountered before, and learned a cool new trick from geeksforgeeks.org to split a string from one column into multiple columns either on spaces or specified characters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN5nJSmooZZi",
        "outputId": "67af88c7-7659-4cdf-b52e-77d0a0ac1788"
      },
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP', 'is', 'short', 'for', 'Natural', 'Language', 'Processing', '.', 'As', 'you', 'probably', 'know', ',', 'computers', 'are', 'not', 'as', 'great', 'at', 'understanding', 'words', 'as', 'they', 'are', 'numbers', '.', 'This', 'is', 'all', 'changing', 'though', 'as', 'advances', 'in', 'NLP', 'are', 'happening', 'everyday', '.', 'The', 'fact', 'that', 'devices', 'like', 'Apple', '’s', 'Siri', 'and', 'Amazon', '’s', 'Alexa', 'can', '(', 'usually', ')', 'comprehend', 'when', 'we', 'ask', 'the', 'weather', ',', 'for', 'directions', ',', 'or', 'to', 'play', 'a', 'certain', 'genre', 'of', 'music', 'are', 'all', 'examples', 'of', 'NLP', '.', 'The', 'spam', 'filter', 'in', 'your', 'email', 'and', 'the', 'spellcheck', 'you', '’ve', 'used', 'since', 'you', 'learned', 'to', 'type', 'in', 'elementary', 'school', 'are', 'some', 'other', 'basic', 'examples', 'of', 'when', 'your', 'computer', 'is', 'understanding', 'language', '.', '\\n\\n', 'As', 'a', 'data', 'scientist', ',', 'we', 'may', 'use', 'NLP', 'for', 'sentiment', 'analysis', '(', 'classifying', 'words', 'to', 'have', 'positive', 'or', 'negative', 'connotation', ')', 'or', 'to', 'make', 'predictions', 'in', 'classification', 'models', ',', 'among', 'other', 'things', '.', 'Typically', ',', 'whether', 'we', '’re', 'given', 'the', 'data', 'or', 'have', 'to', 'scrape', 'it', ',', 'the', 'text', 'will', 'be', 'in', 'its', 'natural', 'human', 'format', 'of', 'sentences', ',', 'paragraphs', ',', 'tweets', ',', 'etc', '.', 'From', 'there', ',', 'before', 'we', 'can', 'dig', 'into', 'analyzing', ',', 'we', 'will', 'have', 'to', 'do', 'some', 'cleaning', 'to', 'break', 'the', 'text', 'down', 'into', 'a', 'format', 'the', 'computer', 'can', 'easily', 'understand', '.', '\\n\\n', 'For', 'this', 'example', ',', 'we', '’re', 'examining', 'a', 'dataset', 'of', 'Amazon', 'products', '/', 'reviews', 'which', 'can', 'be', 'found', 'and', 'downloaded', 'for', 'free', 'on', 'data.world', '.', 'I', '’ll', 'be', 'using', 'Python', 'in', 'Jupyter', 'notebook', '.', '\\n\\n', 'Here', 'are', 'the', 'imports', 'used', ':', '\\n\\n', '(', 'You', 'may', 'need', 'to', 'run', 'nltk.download', '(', ')', 'in', 'a', 'cell', 'if', 'you', '’ve', 'never', 'previously', 'used', 'it', '.', ')', '\\n\\n', 'Read', 'in', 'csv', 'file', ',', 'create', 'DataFrame', '&', 'check', 'shape', '.', 'We', 'are', 'starting', 'out', 'with', '10,000', 'rows', 'and', '17', 'columns', '.', 'Each', 'row', 'is', 'a', 'different', 'product', 'on', 'Amazon', '.', '\\n\\n', 'I', 'conducted', 'some', 'basic', 'data', 'cleaning', 'that', 'I', 'wo', 'n’t', 'go', 'into', 'detail', 'about', 'now', ',', 'but', 'you', 'can', 'read', 'my', 'post', 'about', 'EDA', 'here', 'if', 'you', 'want', 'some', 'tips', '.', '\\n\\n', 'In', 'order', 'to', 'make', 'the', 'dataset', 'more', 'manageable', 'for', 'this', 'example', ',', 'I', 'first', 'dropped', 'columns', 'with', 'too', 'many', 'nulls', 'and', 'then', 'dropped', 'any', 'remaining', 'rows', 'with', 'null', 'values', '.', 'I', 'changed', 'the', 'number_of_reviews', 'column', 'type', 'from', 'object', 'to', 'integer', 'and', 'then', 'created', 'a', 'new', 'DataFrame', 'using', 'only', 'the', 'rows', 'with', 'no', 'more', 'than', '1', 'review', '.', 'My', 'new', 'shape', 'is', '3,705', 'rows', 'and', '10', 'columns', 'and', 'I', 'renamed', 'it', 'reviews_df', '.', '\\n\\n', 'NOTE', ':', 'If', 'we', 'were', 'actually', 'going', 'to', 'use', 'this', 'dataset', 'for', 'analysis', 'or', 'modeling', 'or', 'anything', 'besides', 'a', 'text', 'preprocessing', 'demo', ',', 'I', 'would', 'not', 'recommend', 'eliminating', 'such', 'a', 'large', 'percent', 'of', 'the', 'rows', '.', '\\n\\n', 'The', 'following', 'workflow', 'is', 'what', 'I', 'was', 'taught', 'to', 'use', 'and', 'like', 'using', ',', 'but', 'the', 'steps', 'are', 'just', 'general', 'suggestions', 'to', 'get', 'you', 'started', '.', 'Usually', 'I', 'have', 'to', 'modify', 'and/or', 'expand', 'depending', 'on', 'the', 'text', 'format', '.', '\\n\\n', 'Remove', 'HTML', 'Tokenization', '+', 'Remove', 'punctuation', 'Remove', 'stop', 'words', 'Lemmatization', 'or', 'Stemming', '\\n\\n', 'While', 'cleaning', 'this', 'data', 'I', 'ran', 'into', 'a', 'problem', 'I', 'had', 'not', 'encountered', 'before', ',', 'and', 'learned', 'a', 'cool', 'new', 'trick', 'from', 'geeksforgeeks.org', 'to', 'split', 'a', 'string', 'from', 'one', 'column', 'into', 'multiple', 'columns', 'either', 'on', 'spaces', 'or', 'specified', 'characters', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpCD-UgNocAi"
      },
      "source": [
        "def cleanData(sentence):\n",
        "\t#sentence = re.sub('[^A-Za-z0-9 ]+', '', sentence)\n",
        "\t#sentence filter(None, re.split(\"[.!?\", setence))\n",
        "\tret = []\n",
        "\tsentence = stemmer.stem(sentence)\t\n",
        "\tfor word in sentence.split():\n",
        "\t\tif not word in stopwords:\n",
        "\t\t\tret.append(word)\n",
        "\treturn \" \".join(ret)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGsUKEtwod8-"
      },
      "source": [
        "# Fungsi untuk menghitung cosine similarity\n",
        "# konsep MMR menghiung jarak tiap kalimat kecuali ke dirina sebdiri\n",
        "def calculateSimilarity(sentence, doc):\n",
        "\t\n",
        "\tif doc == []:\n",
        "\t\treturn 0\n",
        "\t\n",
        "\tvocab = {}\n",
        "\t\n",
        "\tfor word in sentence: #Memasukkan kata-kata yg terdapat pada query (sentence) ke dalam vocabulary\n",
        "\t\tvocab[word] = 0\n",
        "\t#menatukan list ang sebelumna adalah string menatukan sebelah kanan\n",
        "\tdocInOneSentence = '';\n",
        "\tfor t in doc:\n",
        "\t\tdocInOneSentence += (t + ' ') #Menggambungkan seluruh kalimat pada list document menjadi satu string atau kalimat\n",
        "\t\tfor word in t.split(): #Memasukkan kata-kata yg terdapat pada tiap kalimat di document ke dalam vocabulary\n",
        "\t\t\tvocab[word]=0\t\n",
        "\t\n",
        "\tcv = CountVectorizer(vocabulary=vocab.keys()) #Kenapa Vocabularynya harus didefinisikan? Karena proses fitur ekstraksinya antara Document dan Query dipisah, agar menghasilkan dimensi vector yg sama maka kita harus mendefinisikan vocebularynya\n",
        "\n",
        "\tdocVector = cv.fit_transform([docInOneSentence]) #Proses Fitur ekstraksi dokumen\n",
        "\tsentenceVector = cv.fit_transform([sentence]) #Proses fitur ekstraksi query\n",
        "\treturn cosine_similarity(docVector, sentenceVector).flatten() #Perhitungan cosine similarity dan kemudian merubahnya menjadi list 1D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNCw8rs5ogrv"
      },
      "source": [
        "\n",
        "\n",
        "raw_texts = texts\n",
        "raw_texts\n",
        "\n",
        "a = raw_texts.split(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8jaaoeBol9r",
        "outputId": "4526110b-7c4c-423a-d9b4-8277bbf9773b"
      },
      "source": [
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP is short for Natural Language Processing',\n",
              " ' As you probably know, computers are not as great at understanding words as they are numbers',\n",
              " ' This is all changing though as advances in NLP are happening everyday',\n",
              " ' The fact that devices like Apple’s Siri and Amazon’s Alexa can (usually) comprehend when we ask the weather, for directions, or to play a certain genre of music are all examples of NLP',\n",
              " ' The spam filter in your email and the spellcheck you’ve used since you learned to type in elementary school are some other basic examples of when your computer is understanding language',\n",
              " '\\n\\nAs a data scientist, we may use NLP for sentiment analysis (classifying words to have positive or negative connotation) or to make predictions in classification models, among other things',\n",
              " ' Typically, whether we’re given the data or have to scrape it, the text will be in its natural human format of sentences, paragraphs, tweets, etc',\n",
              " ' From there, before we can dig into analyzing, we will have to do some cleaning to break the text down into a format the computer can easily understand',\n",
              " '\\n\\nFor this example, we’re examining a dataset of Amazon products/reviews which can be found and downloaded for free on data',\n",
              " 'world',\n",
              " ' I’ll be using Python in Jupyter notebook',\n",
              " '\\n\\nHere are the imports used:\\n\\n(You may need to run nltk',\n",
              " 'download() in a cell if you’ve never previously used it',\n",
              " ')\\n\\nRead in csv file, create DataFrame & check shape',\n",
              " ' We are starting out with 10,000 rows and 17 columns',\n",
              " ' Each row is a different product on Amazon',\n",
              " '\\n\\nI conducted some basic data cleaning that I won’t go into detail about now, but you can read my post about EDA here if you want some tips',\n",
              " '\\n\\nIn order to make the dataset more manageable for this example, I first dropped columns with too many nulls and then dropped any remaining rows with null values',\n",
              " ' I changed the number_of_reviews column type from object to integer and then created a new DataFrame using only the rows with no more than 1 review',\n",
              " ' My new shape is 3,705 rows and 10 columns and I renamed it reviews_df ',\n",
              " '\\n\\nNOTE: If we were actually going to use this dataset for analysis or modeling or anything besides a text preprocessing demo, I would not recommend eliminating such a large percent of the rows',\n",
              " '\\n\\nThe following workflow is what I was taught to use and like using, but the steps are just general suggestions to get you started',\n",
              " ' Usually I have to modify and/or expand depending on the text format',\n",
              " '\\n\\nRemove HTML Tokenization + Remove punctuation Remove stop words Lemmatization or Stemming\\n\\nWhile cleaning this data I ran into a problem I had not encountered before, and learned a cool new trick from geeksforgeeks',\n",
              " 'org to split a string from one column into multiple columns either on spaces or specified characters',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7vZDxIaon7Z"
      },
      "source": [
        "import time\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "sentences = []\n",
        "clean = []\n",
        "originalSentenceOf = {}\n",
        "\n",
        "#Data cleansing\n",
        "parts = raw_texts.split(\".\")\n",
        "for part in parts:\n",
        "  cl = cleanData(part)\n",
        "  sentences.append(part)\n",
        "  clean.append(cl)\n",
        "  originalSentenceOf[cl] = part\n",
        "# for line in texts:\n",
        "# \tparts = line.split('.')\n",
        "# \tfor part in parts:\n",
        "# \t\tcl = cleanData(part)\n",
        "# \t\t#print cl\n",
        "# \t\tsentences.append(part)\n",
        "# \t\tclean.append(cl)\n",
        "# \t\toriginalSentenceOf[cl] = part\t\t\n",
        "\n",
        "setClean = set(clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgXF6OJmosu1",
        "outputId": "bba27a58-dc73-47b9-f0a4-8e847e19849a"
      },
      "source": [
        "setClean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'',\n",
              " ') read csv file, create dataframe & check shap',\n",
              " 'changed number_of_reviews column type object integer created new dataframe using rows 1 review',\n",
              " 'changing though advances nlp happening everyday',\n",
              " 'conducted basic data cleaning go detail now, read post eda want tip',\n",
              " 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th',\n",
              " 'download() cell never previously used',\n",
              " \"example, we're examining dataset amazon products/reviews found downloaded free data\",\n",
              " \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\",\n",
              " 'following workflow taught use like using, steps general suggestions get start',\n",
              " \"i'll using python jupyter notebook\",\n",
              " 'imports used: (you may need run nltk',\n",
              " 'new shape 3,705 rows 10 columns renamed reviews_df',\n",
              " 'nlp short natural language process',\n",
              " 'note: actually going use dataset analysis modeling anything besides text preprocessing demo, would recommend eliminating large percent row',\n",
              " 'order make dataset manageable example, first dropped columns many nulls dropped remaining rows null valu',\n",
              " 'org split string one column multiple columns either spaces specified charact',\n",
              " 'probably know, computers great understanding words numb',\n",
              " 'remove html tokenization + remove punctuation remove stop words lemmatization stemming cleaning data ran problem encountered before, learned cool new trick geeksforgeek',\n",
              " 'row different product amazon',\n",
              " 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag',\n",
              " 'starting 10,000 rows 17 column',\n",
              " 'there, dig analyzing, cleaning break text format computer easily understand',\n",
              " \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\",\n",
              " 'usually modify and/or expand depending text format',\n",
              " 'world'}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK0HamVGotZH",
        "outputId": "367e9c5c-c5b0-46d3-d0db-2a3fb8a9b4df"
      },
      "source": [
        "scores = {}\n",
        "for data in clean:\n",
        "\ttemp_doc = setClean - set([data]) \n",
        "\tscore = calculateSimilarity(data, list(temp_doc))\n",
        "\tscores[data] = score\n",
        "\t#print score\n",
        "\tprint(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'nlp short natural language process': array([0.14664712])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524]), 'imports used: (you may need run nltk': array([0.1098378])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524]), 'imports used: (you may need run nltk': array([0.1098378]), 'download() cell never previously used': array([0.10300524])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524]), 'imports used: (you may need run nltk': array([0.1098378]), 'download() cell never previously used': array([0.10300524]), ') read csv file, create dataframe & check shap': array([0.07302967])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524]), 'imports used: (you may need run nltk': array([0.1098378]), 'download() cell never previously used': array([0.10300524]), ') read csv file, create dataframe & check shap': array([0.07302967]), 'starting 10,000 rows 17 column': array([0.18057878])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524]), 'imports used: (you may need run nltk': array([0.1098378]), 'download() cell never previously used': array([0.10300524]), ') read csv file, create dataframe & check shap': array([0.07302967]), 'starting 10,000 rows 17 column': array([0.18057878]), 'row different product amazon': array([0.10954451])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524]), 'imports used: (you may need run nltk': array([0.1098378]), 'download() cell never previously used': array([0.10300524]), ') read csv file, create dataframe & check shap': array([0.07302967]), 'starting 10,000 rows 17 column': array([0.18057878]), 'row different product amazon': array([0.10954451]), 'conducted basic data cleaning go detail now, read post eda want tip': array([0.21140657])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524]), 'imports used: (you may need run nltk': array([0.1098378]), 'download() cell never previously used': array([0.10300524]), ') read csv file, create dataframe & check shap': array([0.07302967]), 'starting 10,000 rows 17 column': array([0.18057878]), 'row different product amazon': array([0.10954451]), 'conducted basic data cleaning go detail now, read post eda want tip': array([0.21140657]), 'order make dataset manageable example, first dropped columns many nulls dropped remaining rows null valu': array([0.21289852])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524]), 'imports used: (you may need run nltk': array([0.1098378]), 'download() cell never previously used': array([0.10300524]), ') read csv file, create dataframe & check shap': array([0.07302967]), 'starting 10,000 rows 17 column': array([0.18057878]), 'row different product amazon': array([0.10954451]), 'conducted basic data cleaning go detail now, read post eda want tip': array([0.21140657]), 'order make dataset manageable example, first dropped columns many nulls dropped remaining rows null valu': array([0.21289852]), 'changed number_of_reviews column type object integer created new dataframe using rows 1 review': array([0.2396975])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524]), 'imports used: (you may need run nltk': array([0.1098378]), 'download() cell never previously used': array([0.10300524]), ') read csv file, create dataframe & check shap': array([0.07302967]), 'starting 10,000 rows 17 column': array([0.18057878]), 'row different product amazon': array([0.10954451]), 'conducted basic data cleaning go detail now, read post eda want tip': array([0.21140657]), 'order make dataset manageable example, first dropped columns many nulls dropped remaining rows null valu': array([0.21289852]), 'changed number_of_reviews column type object integer created new dataframe using rows 1 review': array([0.2396975]), 'new shape 3,705 rows 10 columns renamed reviews_df': array([0.21270799])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524]), 'imports used: (you may need run nltk': array([0.1098378]), 'download() cell never previously used': array([0.10300524]), ') read csv file, create dataframe & check shap': array([0.07302967]), 'starting 10,000 rows 17 column': array([0.18057878]), 'row different product amazon': array([0.10954451]), 'conducted basic data cleaning go detail now, read post eda want tip': array([0.21140657]), 'order make dataset manageable example, first dropped columns many nulls dropped remaining rows null valu': array([0.21289852]), 'changed number_of_reviews column type object integer created new dataframe using rows 1 review': array([0.2396975]), 'new shape 3,705 rows 10 columns renamed reviews_df': array([0.21270799]), 'note: actually going use dataset analysis modeling anything besides text preprocessing demo, would recommend eliminating large percent row': array([0.21483446])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524]), 'imports used: (you may need run nltk': array([0.1098378]), 'download() cell never previously used': array([0.10300524]), ') read csv file, create dataframe & check shap': array([0.07302967]), 'starting 10,000 rows 17 column': array([0.18057878]), 'row different product amazon': array([0.10954451]), 'conducted basic data cleaning go detail now, read post eda want tip': array([0.21140657]), 'order make dataset manageable example, first dropped columns many nulls dropped remaining rows null valu': array([0.21289852]), 'changed number_of_reviews column type object integer created new dataframe using rows 1 review': array([0.2396975]), 'new shape 3,705 rows 10 columns renamed reviews_df': array([0.21270799]), 'note: actually going use dataset analysis modeling anything besides text preprocessing demo, would recommend eliminating large percent row': array([0.21483446]), 'following workflow taught use like using, steps general suggestions get start': array([0.15130688])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524]), 'imports used: (you may need run nltk': array([0.1098378]), 'download() cell never previously used': array([0.10300524]), ') read csv file, create dataframe & check shap': array([0.07302967]), 'starting 10,000 rows 17 column': array([0.18057878]), 'row different product amazon': array([0.10954451]), 'conducted basic data cleaning go detail now, read post eda want tip': array([0.21140657]), 'order make dataset manageable example, first dropped columns many nulls dropped remaining rows null valu': array([0.21289852]), 'changed number_of_reviews column type object integer created new dataframe using rows 1 review': array([0.2396975]), 'new shape 3,705 rows 10 columns renamed reviews_df': array([0.21270799]), 'note: actually going use dataset analysis modeling anything besides text preprocessing demo, would recommend eliminating large percent row': array([0.21483446]), 'following workflow taught use like using, steps general suggestions get start': array([0.15130688]), 'usually modify and/or expand depending text format': array([0.18480532])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524]), 'imports used: (you may need run nltk': array([0.1098378]), 'download() cell never previously used': array([0.10300524]), ') read csv file, create dataframe & check shap': array([0.07302967]), 'starting 10,000 rows 17 column': array([0.18057878]), 'row different product amazon': array([0.10954451]), 'conducted basic data cleaning go detail now, read post eda want tip': array([0.21140657]), 'order make dataset manageable example, first dropped columns many nulls dropped remaining rows null valu': array([0.21289852]), 'changed number_of_reviews column type object integer created new dataframe using rows 1 review': array([0.2396975]), 'new shape 3,705 rows 10 columns renamed reviews_df': array([0.21270799]), 'note: actually going use dataset analysis modeling anything besides text preprocessing demo, would recommend eliminating large percent row': array([0.21483446]), 'following workflow taught use like using, steps general suggestions get start': array([0.15130688]), 'usually modify and/or expand depending text format': array([0.18480532]), 'remove html tokenization + remove punctuation remove stop words lemmatization stemming cleaning data ran problem encountered before, learned cool new trick geeksforgeek': array([0.26797405])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524]), 'imports used: (you may need run nltk': array([0.1098378]), 'download() cell never previously used': array([0.10300524]), ') read csv file, create dataframe & check shap': array([0.07302967]), 'starting 10,000 rows 17 column': array([0.18057878]), 'row different product amazon': array([0.10954451]), 'conducted basic data cleaning go detail now, read post eda want tip': array([0.21140657]), 'order make dataset manageable example, first dropped columns many nulls dropped remaining rows null valu': array([0.21289852]), 'changed number_of_reviews column type object integer created new dataframe using rows 1 review': array([0.2396975]), 'new shape 3,705 rows 10 columns renamed reviews_df': array([0.21270799]), 'note: actually going use dataset analysis modeling anything besides text preprocessing demo, would recommend eliminating large percent row': array([0.21483446]), 'following workflow taught use like using, steps general suggestions get start': array([0.15130688]), 'usually modify and/or expand depending text format': array([0.18480532]), 'remove html tokenization + remove punctuation remove stop words lemmatization stemming cleaning data ran problem encountered before, learned cool new trick geeksforgeek': array([0.26797405]), 'org split string one column multiple columns either spaces specified charact': array([0.14784425])}\n",
            "{'nlp short natural language process': array([0.14664712]), 'probably know, computers great understanding words numb': array([0.1098378]), 'changing though advances nlp happening everyday': array([0.15533411]), \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]), 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]), 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]), \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]), 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]), \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]), 'world': array([0.]), \"i'll using python jupyter notebook\": array([0.10300524]), 'imports used: (you may need run nltk': array([0.1098378]), 'download() cell never previously used': array([0.10300524]), ') read csv file, create dataframe & check shap': array([0.07302967]), 'starting 10,000 rows 17 column': array([0.18057878]), 'row different product amazon': array([0.10954451]), 'conducted basic data cleaning go detail now, read post eda want tip': array([0.21140657]), 'order make dataset manageable example, first dropped columns many nulls dropped remaining rows null valu': array([0.21289852]), 'changed number_of_reviews column type object integer created new dataframe using rows 1 review': array([0.2396975]), 'new shape 3,705 rows 10 columns renamed reviews_df': array([0.21270799]), 'note: actually going use dataset analysis modeling anything besides text preprocessing demo, would recommend eliminating large percent row': array([0.21483446]), 'following workflow taught use like using, steps general suggestions get start': array([0.15130688]), 'usually modify and/or expand depending text format': array([0.18480532]), 'remove html tokenization + remove punctuation remove stop words lemmatization stemming cleaning data ran problem encountered before, learned cool new trick geeksforgeek': array([0.26797405]), 'org split string one column multiple columns either spaces specified charact': array([0.14784425]), '': array([0.])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBhibOFbovtt",
        "outputId": "9edebff6-4d6a-4d58-95da-87d0a3a20d7e"
      },
      "source": [
        "scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'': array([0.]),\n",
              " ') read csv file, create dataframe & check shap': array([0.07302967]),\n",
              " 'changed number_of_reviews column type object integer created new dataframe using rows 1 review': array([0.2396975]),\n",
              " 'changing though advances nlp happening everyday': array([0.15533411]),\n",
              " 'conducted basic data cleaning go detail now, read post eda want tip': array([0.21140657]),\n",
              " 'data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th': array([0.28571429]),\n",
              " 'download() cell never previously used': array([0.10300524]),\n",
              " \"example, we're examining dataset amazon products/reviews found downloaded free data\": array([0.24275885]),\n",
              " \"fact devices like apple's siri amazon's alexa (usually) comprehend ask weather, directions, play certain genre music examples nlp\": array([0.19015308]),\n",
              " 'following workflow taught use like using, steps general suggestions get start': array([0.15130688]),\n",
              " \"i'll using python jupyter notebook\": array([0.10300524]),\n",
              " 'imports used: (you may need run nltk': array([0.1098378]),\n",
              " 'new shape 3,705 rows 10 columns renamed reviews_df': array([0.21270799]),\n",
              " 'nlp short natural language process': array([0.14664712]),\n",
              " 'note: actually going use dataset analysis modeling anything besides text preprocessing demo, would recommend eliminating large percent row': array([0.21483446]),\n",
              " 'order make dataset manageable example, first dropped columns many nulls dropped remaining rows null valu': array([0.21289852]),\n",
              " 'org split string one column multiple columns either spaces specified charact': array([0.14784425]),\n",
              " 'probably know, computers great understanding words numb': array([0.1098378]),\n",
              " 'remove html tokenization + remove punctuation remove stop words lemmatization stemming cleaning data ran problem encountered before, learned cool new trick geeksforgeek': array([0.26797405]),\n",
              " 'row different product amazon': array([0.10954451]),\n",
              " 'spam filter email spellcheck used since learned type elementary school basic examples computer understanding languag': array([0.16070868]),\n",
              " 'starting 10,000 rows 17 column': array([0.18057878]),\n",
              " 'there, dig analyzing, cleaning break text format computer easily understand': array([0.21052632]),\n",
              " \"typically, whether we're given data scrape it, text natural human format sentences, paragraphs, tweets, etc\": array([0.26499947]),\n",
              " 'usually modify and/or expand depending text format': array([0.18480532]),\n",
              " 'world': array([0.])}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twe0oPnro1l6",
        "outputId": "28f3ab72-287b-4b04-c046-fdbac01d91a0"
      },
      "source": [
        "n = 20 * len(sentences) / 100 #Set mau seberapa banyak hasil dari summarisasi\n",
        "alpha = 0.5 #Set Nilai Alpha\n",
        "summarySet = [] #Set list kosong untuk menyimpan hasil teks summarisasi\n",
        "\n",
        "while n > 0: #melakukan looping selama nilai n (banyak kalimat yg akan diekstraksi) lebih besar dari 0\n",
        "\tmmr = {} #Inisialisasi dictionary untuk menyimpan nilai MMR \n",
        "\t#kurangkan dengan set summary\n",
        "  # Removed dict.iteritems(), dict.iterkeys(), and dict.itervalues().\n",
        "  # Instead: use dict.items(), dict.keys(), and dict.values() respectively.\n",
        "\n",
        "\tfor sentence in scores.keys():\n",
        "\t\tif not sentence in summarySet:\n",
        "\t\t\tmmr[sentence] = alpha * scores[sentence] - (1-alpha) * calculateSimilarity(sentence, summarySet) #Mencari nilai MMR tiap kalimat yg ada pada document terhadap text yg sudah dirangkum\n",
        "\t \n",
        "\tselected = max(mmr.items(), key=operator.itemgetter(1))[0]\t#Memilih teks dengan nilai MMR paling besar dari proses sebelumnya\n",
        "\t\n",
        "\tsummarySet.append(selected) #Menyimpan text tersebut kedalam list yg menyimpan hasil text sunmarisasi\n",
        "\tprint(summarySet) #Melihat text apa saja yg sudah masuk kedalam hasil summarisasi\n",
        "\tn -= 1\n",
        "\n",
        "#rint str(time.time() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th']\n",
            "['data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th', 'changed number_of_reviews column type object integer created new dataframe using rows 1 review']\n",
            "['data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th', 'changed number_of_reviews column type object integer created new dataframe using rows 1 review', 'there, dig analyzing, cleaning break text format computer easily understand']\n",
            "['data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th', 'changed number_of_reviews column type object integer created new dataframe using rows 1 review', 'there, dig analyzing, cleaning break text format computer easily understand', 'row different product amazon']\n",
            "['data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th', 'changed number_of_reviews column type object integer created new dataframe using rows 1 review', 'there, dig analyzing, cleaning break text format computer easily understand', 'row different product amazon', 'download() cell never previously used']\n",
            "['data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th', 'changed number_of_reviews column type object integer created new dataframe using rows 1 review', 'there, dig analyzing, cleaning break text format computer easily understand', 'row different product amazon', 'download() cell never previously used', \"example, we're examining dataset amazon products/reviews found downloaded free data\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYQA2d_To2JO",
        "outputId": "8d9def67-ee3d-4700-fa0a-78b30c2c925a"
      },
      "source": [
        "summarySet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data scientist, may use nlp sentiment analysis (classifying words positive negative connotation) make predictions classification models, among th',\n",
              " 'changed number_of_reviews column type object integer created new dataframe using rows 1 review',\n",
              " 'there, dig analyzing, cleaning break text format computer easily understand',\n",
              " 'row different product amazon',\n",
              " 'download() cell never previously used',\n",
              " \"example, we're examining dataset amazon products/reviews found downloaded free data\"]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CXeWWfRo42o",
        "outputId": "f089fbda-4528-4b60-d97d-b1f751eb9b02"
      },
      "source": [
        "print ('\\nSummary (hasil teks yang diringkas):\\n')\n",
        "\n",
        "for sentence in summarySet: #Tadi yang kita simpan didalam summarySet adalah text yg sudah disummarisasi tetapi itu hasil dari proses cleaning, disini kita mengambil text aslinya dari dictionary sentence\n",
        "\tprint (originalSentenceOf[sentence].lstrip(' '))\n",
        "print ('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary (hasil teks yang diringkas):\n",
            "\n",
            "\n",
            "\n",
            "As a data scientist, we may use NLP for sentiment analysis (classifying words to have positive or negative connotation) or to make predictions in classification models, among other things\n",
            "I changed the number_of_reviews column type from object to integer and then created a new DataFrame using only the rows with no more than 1 review\n",
            "From there, before we can dig into analyzing, we will have to do some cleaning to break the text down into a format the computer can easily understand\n",
            "Each row is a different product on Amazon\n",
            "download() in a cell if you’ve never previously used it\n",
            "\n",
            "\n",
            "For this example, we’re examining a dataset of Amazon products/reviews which can be found and downloaded for free on data\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rewDIpGAo65T",
        "outputId": "9f1a86da-5c77-401c-f7e4-74569d17a068"
      },
      "source": [
        "print ('=============================================================')\n",
        "print ('\\nOriginal Passages (Teks Asli):\\n')\n",
        "from termcolor import colored # tools untuk membuat highlight merah\n",
        "\n",
        "for sentence in clean:\n",
        "\tif sentence in summarySet:\n",
        "\t\tprint (colored(originalSentenceOf[sentence].lstrip(' '), 'yellow'))\n",
        "\telse:\n",
        "\t\tprint (originalSentenceOf[sentence].lstrip(' '))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================================================\n",
            "\n",
            "Original Passages (Teks Asli):\n",
            "\n",
            "NLP is short for Natural Language Processing\n",
            "As you probably know, computers are not as great at understanding words as they are numbers\n",
            "This is all changing though as advances in NLP are happening everyday\n",
            "The fact that devices like Apple’s Siri and Amazon’s Alexa can (usually) comprehend when we ask the weather, for directions, or to play a certain genre of music are all examples of NLP\n",
            "The spam filter in your email and the spellcheck you’ve used since you learned to type in elementary school are some other basic examples of when your computer is understanding language\n",
            "\u001b[33m\n",
            "\n",
            "As a data scientist, we may use NLP for sentiment analysis (classifying words to have positive or negative connotation) or to make predictions in classification models, among other things\u001b[0m\n",
            "Typically, whether we’re given the data or have to scrape it, the text will be in its natural human format of sentences, paragraphs, tweets, etc\n",
            "\u001b[33mFrom there, before we can dig into analyzing, we will have to do some cleaning to break the text down into a format the computer can easily understand\u001b[0m\n",
            "\u001b[33m\n",
            "\n",
            "For this example, we’re examining a dataset of Amazon products/reviews which can be found and downloaded for free on data\u001b[0m\n",
            "world\n",
            "I’ll be using Python in Jupyter notebook\n",
            "\n",
            "\n",
            "Here are the imports used:\n",
            "\n",
            "(You may need to run nltk\n",
            "\u001b[33mdownload() in a cell if you’ve never previously used it\u001b[0m\n",
            ")\n",
            "\n",
            "Read in csv file, create DataFrame & check shape\n",
            "We are starting out with 10,000 rows and 17 columns\n",
            "\u001b[33mEach row is a different product on Amazon\u001b[0m\n",
            "\n",
            "\n",
            "I conducted some basic data cleaning that I won’t go into detail about now, but you can read my post about EDA here if you want some tips\n",
            "\n",
            "\n",
            "In order to make the dataset more manageable for this example, I first dropped columns with too many nulls and then dropped any remaining rows with null values\n",
            "\u001b[33mI changed the number_of_reviews column type from object to integer and then created a new DataFrame using only the rows with no more than 1 review\u001b[0m\n",
            "My new shape is 3,705 rows and 10 columns and I renamed it reviews_df \n",
            "\n",
            "\n",
            "NOTE: If we were actually going to use this dataset for analysis or modeling or anything besides a text preprocessing demo, I would not recommend eliminating such a large percent of the rows\n",
            "\n",
            "\n",
            "The following workflow is what I was taught to use and like using, but the steps are just general suggestions to get you started\n",
            "Usually I have to modify and/or expand depending on the text format\n",
            "\n",
            "\n",
            "Remove HTML Tokenization + Remove punctuation Remove stop words Lemmatization or Stemming\n",
            "\n",
            "While cleaning this data I ran into a problem I had not encountered before, and learned a cool new trick from geeksforgeeks\n",
            "org to split a string from one column into multiple columns either on spaces or specified characters\n",
            "\n"
          ]
        }
      ]
    }
  ]
}